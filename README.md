
## 给你海量数据数字，请找出里面的中位数
这个问题可以转化为海量数字的 Top K问题。
先假设数字为 4 byte
1k = 250个数字
1M = 25W个数字
1G = 2.5MW个数字
### 思路
先假设数字范围随机，数字重复率很低
1. 排序，假若对这些数字还有其他处理需求。直接读入内存，使用内省排序，然后取中位数。
2. 快速划分，仅仅只是做一次这种操作。直接读入内存，使用快速排序的 `partition` 可以在 `klogN`的情况下找到中位数。
3. 假若数字范围有限，或者重复数字超过 `50%`，可以使用 `bucket` 计数，在扫描数字超过一次，根据 `bucket` 的计数，可以确定数据的中位数。
4. 假如数据重复较多，可以使用 `HashMap` 或红黑树存储，然后便利该数据结构得到答案。
5. 根据这种想法，我们是不是可以使用树的方式存储二进制数据，这样可以减少内存。也可以采用 `bitmap`不过对于重复数据要另做处理。

假如数据较大，内存放不下
1. 使用外部排序，这种方法是后续对文件还有其他处理需求的情况，时间复杂度为 `Okn/klogn/k）`
2. 快速划分，借助外部排序的方法，对文件做外部划分.
3. 既然这里只是考虑找中位数，我们可以采用 `bucket` 的思想，在大数据的情况下，使用 `bitmap`，然后扫描该 `bitmap`，可以找出对弈的 `top k`。


## 给你两个文件，每个文件10G，里面每一行都URL，请找出里面的相同的URL
### 思考
- 这个操作是每天一次性的操作，还是可能数据文件不断增加，流式操作？
- 一个 URL 长度为 2048 byte
- URL 请求，必定存在大量的重复 URL，可以考虑对 URL 进行去重再压缩
- URL 本身有一定的规律，可以考虑采用URL压缩算法，内存的使用
 - 查询到了一篇论文 [In-memory URL Compression](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.8471&rep=rep1&type=pdf)，该算法可以压缩 URL 节省50%以上内存。

1. 排序，先对这两个文件做外部排序，`merge` 阶段可以直接合并该文件中重复的 `URL`，然后可以采用二分查找相同的 URL 行
2. 单机 `MapReduce`，将每行URL进行哈希映射，映射到十几个小文件中，再找到哈希取模相同的文件，若文件较大先去重，否则直接Hash+红黑树进行比较，找到两个文件的重复URL
3. B+树，假如这种操作很频繁，那么是否可以对其中一个文件建立B+树，内存做LRU查询缓存，分别查找
4. URL 的重复率一般很高，这时可以采用布隆过滤器，先对文件A建立布隆过滤器，然后文件B做碰撞查询，将重复的URL输出到 `tmpFile`，`tmpFile`应该足够放入内存，然后就可以在内存中简单的处理了
5. 考虑到布隆过滤器的思想，我们是否可以使用 `bitmap` 或者 压缩的 `HashMap` 来进行统计？

### 扩展思考
该题目的要求是找出相同的URL，那么是否还有进一步的需求找到相同的URL，并统计相同的 URL 的总条数呢？
这样就需要对算法做进一步扩展
